{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "La gestión de ausencias y licencias en una empresa es crucial para mantener la operación sin problemas. El sistema de recursos humanos se enfrenta al reto de decidir aprobar o rechazar solicitudes, buscando equilibrar la flexibilidad de los empleados con la necesidad de mantener una programación eficiente. Utilizando métodos como el Q-learning, el sistema puede aprender a tomar decisiones más efectivas, adaptándose a los cambios en el personal y mejorando la eficiencia operativa."
      ],
      "metadata": {
        "id": "ma_JmeVPFpJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tAyFfrgp8Ot8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establece los parámetros clave del algoritmo Q-learning.\n",
        "\n",
        "**alpha**: Tasa de aprendizaje, que determina cuánto se actualizan los valores Q en cada iteración.\n",
        "\n",
        "**gamma**: Factor de descuento, que ajusta la importancia de las recompensas futuras en la actualización de los valores Q.\n",
        "\n",
        "**epsilon**: Tasa de exploración, que controla la probabilidad de tomar una acción aleatoria en lugar de la mejor acción conocida."
      ],
      "metadata": {
        "id": "zPtG4mIg8R6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1  # Tasa de aprendizaje\n",
        "gamma = 0.9  # Factor de descuento\n",
        "epsilon = 0.1  # Tasa de exploración"
      ],
      "metadata": {
        "id": "1R8Ay9qQ8TSn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define una lista de estados posibles y acciones posibles. Estos son ficticios y simplificados para este ejemplo.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJEmBQ3S9aM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estados = [\"Carga Baja\", \"Carga Media\", \"Carga Alta\"]\n",
        "acciones = [\"Aprobar\", \"Rechazar\"]\n"
      ],
      "metadata": {
        "id": "Xkv9oJOj8Whi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializa la tabla Q con ceros. La tabla Q es donde se almacenan los valores que el agente utiliza para tomar decisiones."
      ],
      "metadata": {
        "id": "sZxvVgr-9fYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros((len(estados), len(acciones)))\n"
      ],
      "metadata": {
        "id": "qFZykKJ68XT5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define una función ficticia para calcular la recompensa en función del estado y la acción. Es una función de recompensa simple para ilustrar el concepto."
      ],
      "metadata": {
        "id": "TCwEpN6G9hat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_recompensa(estado, accion):\n",
        "    if estado == \"Carga Baja\" and accion == \"Aprobar\":\n",
        "        return 1\n",
        "    elif estado == \"Carga Alta\" and accion == \"Rechazar\":\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n"
      ],
      "metadata": {
        "id": "r6RQTVAd8Yfo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define una función para que el agente seleccione una acción. Utiliza la política epsilon-greedy, donde hay una probabilidad epsilon de seleccionar una acción aleatoria y 1 - epsilon de seleccionar la mejor acción conocida."
      ],
      "metadata": {
        "id": "LoBQlxj-9jRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seleccionar_accion(estado):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(acciones)\n",
        "    else:\n",
        "        indice_accion_optima = np.argmax(Q[estados.index(estado)])\n",
        "        return acciones[indice_accion_optima]\n"
      ],
      "metadata": {
        "id": "BWM1eNWx8Z1h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa el proceso de entrenamiento del agente mediante iteraciones. En cada iteración, el agente selecciona acciones, aplica estas acciones en el entorno, observa recompensas y actualiza los valores Q utilizando la ecuación de Bellman."
      ],
      "metadata": {
        "id": "ZofXc8mB9lO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas_entrenamiento = 1000\n",
        "\n",
        "for _ in range(epocas_entrenamiento):\n",
        "    estado_actual = np.random.choice(estados)\n",
        "\n",
        "    while True:\n",
        "        accion_seleccionada = seleccionar_accion(estado_actual)\n",
        "        recompensa = obtener_recompensa(estado_actual, accion_seleccionada)\n",
        "        nuevo_estado = np.random.choice(estados)\n",
        "\n",
        "        Q[estados.index(estado_actual), acciones.index(accion_seleccionada)] += \\\n",
        "            alpha * (recompensa + gamma * np.max(Q[estados.index(nuevo_estado)]) - Q[estados.index(estado_actual), acciones.index(accion_seleccionada)])\n",
        "\n",
        "        if nuevo_estado == \"Carga Baja\" or nuevo_estado == \"Carga Alta\":\n",
        "            break\n",
        "\n",
        "        estado_actual = nuevo_estado\n"
      ],
      "metadata": {
        "id": "5ia5j-uo8bwI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evalúa el agente utilizando la política aprendida después del entrenamiento. Imprime la acción óptima para cada estado según los valores Q aprendidos."
      ],
      "metadata": {
        "id": "oBtS5EX39nhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estado_actual = \"Carga Media\"\n",
        "\n",
        "print(\"Política final del agente:\")\n",
        "for estado in estados:\n",
        "    accion_optima = acciones[np.argmax(Q[estados.index(estado)])]\n",
        "    print(f\"Estado: {estado}, Acción Óptima: {accion_optima}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtjeeoMA8dLQ",
        "outputId": "0695284e-aa3c-4feb-b348-4145a28c71d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Política final del agente:\n",
            "Estado: Carga Baja, Acción Óptima: Aprobar\n",
            "Estado: Carga Media, Acción Óptima: Aprobar\n",
            "Estado: Carga Alta, Acción Óptima: Rechazar\n"
          ]
        }
      ]
    }
  ]
}